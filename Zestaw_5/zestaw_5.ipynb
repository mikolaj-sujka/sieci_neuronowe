{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zestaw 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaki jest paradygmat uczenia sieci RNN?\n",
    "- Sieci RNN uczą się na danych sekwencyjnych, gdzie bieżące wyjście zależy nie tylko od bieżącego wejścia, ale także od poprzednich stanów. Uczenie odbywa się za pomocą Backpropagation Through Time (BPTT), czyli rozszerzenia algorytmu wstecznej propagacji na sekwencje czasowe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jak można alternatywnie definiować problemy rozwiązywane siecią rekurencyjną?\n",
    "Sieci RNN mogą być używane do różnych zadań, takich jak:\n",
    "\n",
    "- Predykcja szeregów czasowych: Prognozowanie przyszłych wartości na podstawie przeszłych danych.\n",
    "- Klasyfikacja sekwencji: Określenie kategorii całej sekwencji, np. klasyfikacja emocji w zdaniu.\n",
    "- Generowanie sekwencji: Tworzenie nowych danych sekwencyjnych, np. generowanie tekstu.\n",
    "- Tłumaczenie maszynowe: Przekładanie tekstu z jednego języka na inny.\n",
    "- Rozpoznawanie mowy: Konwersja mowy na tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trenowanie modelu: RNN\n",
      "Epoch 1/20\n",
      "23/23 [==============================] - 2s 18ms/step - loss: 0.0690 - val_loss: 0.0315\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0188 - val_loss: 0.0149\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0136 - val_loss: 0.0143\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0138 - val_loss: 0.0192\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.0140\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0133 - val_loss: 0.0137\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0116 - val_loss: 0.0131\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0130 - val_loss: 0.0128\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0119 - val_loss: 0.0123\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0122 - val_loss: 0.0138\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0113 - val_loss: 0.0120\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0121\n",
      "Epoch 13/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0116 - val_loss: 0.0136\n",
      "Epoch 17/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0132\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0127\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0126\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0116\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0119\n",
      "Test Loss for RNN: 0.0118815116584301\n",
      "\n",
      "Trenowanie modelu: LSTM\n",
      "Epoch 1/20\n",
      "23/23 [==============================] - 3s 31ms/step - loss: 0.3497 - val_loss: 0.2064\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1061 - val_loss: 0.0376\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0177 - val_loss: 0.0164\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0127 - val_loss: 0.0155\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0117 - val_loss: 0.0150\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0127 - val_loss: 0.0152\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0119 - val_loss: 0.0152\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0116 - val_loss: 0.0144\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0114 - val_loss: 0.0148\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0129 - val_loss: 0.0147\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0117 - val_loss: 0.0143\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.0121 - val_loss: 0.0151\n",
      "Epoch 13/20\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0117 - val_loss: 0.0141\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0113 - val_loss: 0.0143\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0118 - val_loss: 0.0142\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0114 - val_loss: 0.0141\n",
      "Epoch 17/20\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0113 - val_loss: 0.0141\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0114 - val_loss: 0.0143\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0128 - val_loss: 0.0148\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0114 - val_loss: 0.0143\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0123\n",
      "Test Loss for LSTM: 0.01233304850757122\n",
      "\n",
      "Trenowanie modelu: GRU\n",
      "Epoch 1/20\n",
      "23/23 [==============================] - 2s 27ms/step - loss: 0.1161 - val_loss: 0.0626\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0506 - val_loss: 0.0445\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0360 - val_loss: 0.0315\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0260 - val_loss: 0.0237\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0177 - val_loss: 0.0175\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0137 - val_loss: 0.0179\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0141 - val_loss: 0.0169\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0139 - val_loss: 0.0166\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0131 - val_loss: 0.0170\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0132 - val_loss: 0.0169\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0136 - val_loss: 0.0166\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0130 - val_loss: 0.0161\n",
      "Epoch 13/20\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0166\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0144 - val_loss: 0.0172\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0136 - val_loss: 0.0158\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0125 - val_loss: 0.0159\n",
      "Epoch 17/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0129 - val_loss: 0.0157\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0138 - val_loss: 0.0160\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0152 - val_loss: 0.0162\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.0127 - val_loss: 0.0169\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0145\n",
      "Test Loss for GRU: 0.014546386897563934\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense\n",
    "\n",
    "# Generowanie przykładowych danych sekwencyjnych\n",
    "def generate_sequence(seq_length=1000):\n",
    "    x = np.linspace(0, 100, seq_length)\n",
    "    y = np.sin(x) + 0.1 * np.random.randn(seq_length)\n",
    "    return y\n",
    "\n",
    "data = generate_sequence()\n",
    "\n",
    "# Przygotowanie danych do modelu RNN\n",
    "def create_dataset(data, time_steps=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i+time_steps])\n",
    "        y.append(data[i+time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_steps = 20\n",
    "X, y = create_dataset(data, time_steps)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))  # [samples, time_steps, features]\n",
    "\n",
    "# Podział na zbiór treningowy i testowy\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Funkcja do budowy modelu RNN\n",
    "def build_rnn(model_type='RNN'):\n",
    "    model = Sequential()\n",
    "    if model_type == 'RNN':\n",
    "        model.add(SimpleRNN(50, activation='tanh', input_shape=(time_steps, 1)))\n",
    "    elif model_type == 'LSTM':\n",
    "        model.add(LSTM(50, activation='tanh', input_shape=(time_steps, 1)))\n",
    "    elif model_type == 'GRU':\n",
    "        model.add(GRU(50, activation='tanh', input_shape=(time_steps, 1)))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Trenowanie i ocena modeli\n",
    "for model_type in ['RNN', 'LSTM', 'GRU']:\n",
    "    print(f\"\\nTrenowanie modelu: {model_type}\")\n",
    "    model = build_rnn(model_type)\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Loss for {model_type}: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Generowanie danych: suma dwóch sinusoid z różnymi częstotliwościami\n",
    "def generate_time_series(seq_length=2000):\n",
    "    x = np.linspace(0, 100, seq_length)\n",
    "    y = np.sin(x) + 0.5 * np.sin(5 * x) + 0.1 * np.random.randn(seq_length)\n",
    "    return y\n",
    "\n",
    "data = generate_time_series()\n",
    "\n",
    "# Przygotowanie danych\n",
    "def create_dataset(data, time_steps=50):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i+time_steps])\n",
    "        y.append(data[i+time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_steps = 50\n",
    "X, y = create_dataset(data, time_steps)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Podział na trening i test\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Budowa modelu LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='tanh', input_shape=(time_steps, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Trenowanie modelu\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Ocena modelu\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "# Przewidywanie i wizualizacja\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(y_test, label='Rzeczywiste')\n",
    "plt.plot(y_pred, label='Przewidywane')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Parametry\n",
    "vocab_size = 10000\n",
    "max_length = 200\n",
    "embedding_dim = 128\n",
    "\n",
    "# Ładowanie danych\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Padding sekwencji\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# Budowa modelu\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Kompilacja modelu\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Trenowanie modelu\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Ocena modelu\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jak działają transformery?\n",
    "Transformery to architektura sieci neuronowej, która zrewolucjonizowała przetwarzanie języka naturalnego (NLP) dzięki swojej efektywności w modelowaniu długich zależności w danych sekwencyjnych. Kluczowymi elementami transformera są:\n",
    "- Mechanizm Uwag (Attention Mechanism)\n",
    "- Struktura Enkoder- Dekoder\n",
    "- Warstwy Normalizacji (Normalization Layers) i Reszty (Residual Connections)\n",
    "- Zaletę transformera stanowi to, że nie wymaga on rekurencyjnych lub splotowych warstw, co pozwala na równoległe przetwarzanie danych sekwencyjnych.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Ładowanie danych\n",
    "path = tf.keras.utils.get_file('alice.txt', 'https://www.gutenberg.org/files/11/11-0.txt')\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()\n",
    "\n",
    "# Tokenizacja\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Tworzenie sekwencji\n",
    "sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        sequences.append(n_gram_sequence)\n",
    "\n",
    "# Padding sekwencji\n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Rozdzielenie na X i y\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Budowa modelu\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Kompilacja modelu\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Trenowanie modelu\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "# Funkcja do generowania tekstu\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word = np.argmax(predicted, axis=1)[0]\n",
    "        output_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "# Generowanie tekstu\n",
    "print(generate_text(\"alice was beginning to get very tired\", 20, model, max_sequence_len))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Przykładowe dane (bardzo mały zbiór dla uproszczenia)\n",
    "english_sentences = [\n",
    "    'hello',\n",
    "    'how are you',\n",
    "    'good morning',\n",
    "    'good night',\n",
    "    'thank you',\n",
    "    'yes',\n",
    "    'no'\n",
    "]\n",
    "\n",
    "spanish_sentences = [\n",
    "    'hola',\n",
    "    'cómo estás',\n",
    "    'buenos días',\n",
    "    'buenas noches',\n",
    "    'gracias',\n",
    "    'sí',\n",
    "    'no'\n",
    "]\n",
    "\n",
    "# Tokenizacja\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(english_sentences)\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "spa_tokenizer = Tokenizer()\n",
    "spa_tokenizer.fit_on_texts(spanish_sentences)\n",
    "spa_vocab_size = len(spa_tokenizer.word_index) + 1\n",
    "\n",
    "# Dodanie tokenów start i stop\n",
    "spanish_sentences = ['\\t ' + sentence + ' \\n' for sentence in spanish_sentences]\n",
    "\n",
    "# Tworzenie sekwencji\n",
    "encoder_input_data = pad_sequences(eng_tokenizer.texts_to_sequences(english_sentences), padding='post')\n",
    "decoder_input_data = pad_sequences(spa_tokenizer.texts_to_sequences([s.split()[0] for s in spanish_sentences]), padding='post')\n",
    "decoder_target_data = pad_sequences(spa_tokenizer.texts_to_sequences([s for s in spanish_sentences]), padding='post')\n",
    "\n",
    "# Parametry modelu\n",
    "latent_dim = 256\n",
    "\n",
    "# Enkoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Dense(latent_dim, activation='relu')(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Dekoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb = Dense(latent_dim, activation='relu')(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(spa_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model trenowania\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Przygotowanie danych do trenowania\n",
    "decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
    "\n",
    "# Trenowanie modelu\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          epochs=500,\n",
    "          batch_size=2)\n",
    "\n",
    "# Definicja modelu enkodera\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Definicja modelu dekodera\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(dec_emb, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Funkcja tłumaczenia\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.array([spa_tokenizer.word_index['\\t']])\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = None\n",
    "        for word, index in spa_tokenizer.word_index.items():\n",
    "            if index == sampled_token_index:\n",
    "                sampled_word = word\n",
    "                break\n",
    "        if sampled_word == '\\n' or len(decoded_sentence.split()) > 10:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += ' ' + sampled_word\n",
    "            target_seq = np.array([sampled_token_index])\n",
    "            states_value = [h, c]\n",
    "    return decoded_sentence\n",
    "\n",
    "# Testowanie modelu\n",
    "for seq_index in range(len(english_sentences)):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(f'Input: {english_sentences[seq_index]}')\n",
    "    print(f'Decoded: {decoded_sentence}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem LLMs (Large Language Models)\n",
    "a. Wprowadzenie \n",
    "- LLMs (Large Language Models) to duże modele językowe o miliardach parametrów, trenowane na ogromnych zbiorach danych tekstowych. Przykładami są GPT-3, GPT-4, BERT, T5 itp. Są one zdolne do wykonywania szerokiego zakresu zadań NLP, takich jak tłumaczenie, generowanie tekstu, odpowiadanie na pytania, analiza sentymentu i wiele innych.\n",
    "\n",
    "b. Problemy związane z LLMs\n",
    "- Zapotrzebowanie na Zasoby Obliczeniowe:\n",
    "- Energochłonność\n",
    "- Bias i Etyka\n",
    "- Trudność w interpretacji\n",
    "- Bezpieczeństwo i Prywatność\n",
    "- Aktualność wiedzy\n",
    "- Złożoność implementacji\n",
    "\n",
    "c. Przykładowe rozwiązania problemów\n",
    "- Modele Lżejsze: Tworzenie mniejszych, bardziej efektywnych modeli, które zachowują wydajność dużych LLMs przy mniejszych wymaganiach obliczeniowych.\n",
    "- Transfer Learning i Fine-Tuning: Wykorzystanie wcześniej wytrenowanych modeli i dostosowywanie ich do specyficznych zadań, co zmniejsza potrzebę trenowania modeli od podstaw.\n",
    "- Mechanizmy Odpowiedzialnego AI: Wdrażanie technik mających na celu ograniczenie biasu, zapewnienie interpretowalności i zwiększenie bezpieczeństwa modeli.\n",
    "- Zrównoważony Rozwój: Prace nad zmniejszeniem śladu węglowego związanego z treningiem dużych modeli poprzez optymalizację algorytmów i wykorzystanie bardziej efektywnych technologii sprzętowych."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
